{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx-bUBtGkO-M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cAZGMovTkWJ4"
   },
   "outputs": [],
   "source": [
    "#Model and Params\n",
    "#Numbers\n",
    "NumTrain=500\n",
    "NT=80\n",
    "dt=1/NT\n",
    "sigma=0.03\n",
    "\n",
    "#Forward Loss\n",
    "forward_losses = []\n",
    "#Network Class for FBSDE\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_outputs):\n",
    "        '''\n",
    "        lr: learning rate\n",
    "        '''\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        #Pass input parameters\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_out = n_outputs\n",
    "\n",
    "        #Construct network\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_out)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x= F.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "    \n",
    "## Functions\n",
    "def Sample_Init(N,mean=0,sd=0.1):\n",
    "    '''\n",
    "    Generate N samples of x0\n",
    "    '''\n",
    "    xi = np.random.normal(mean,sd,size=N)\n",
    "\n",
    "    return torch.FloatTensor(xi).view(-1,1)\n",
    "\n",
    "def SampleBMIncr(T, Npaths, Nsteps):\n",
    "    # Returns Matrix of Dimension Npaths x Nsteps With Sample Increments of of BM\n",
    "    # Here an increment is of the form dB\n",
    "    dt = T / Nsteps\n",
    "    dB = np.sqrt(dt) * np.random.randn(Npaths, Nsteps)\n",
    "    return torch.FloatTensor(dB)\n",
    "\n",
    "def target(x,sigma=sigma):\n",
    "    x=x.detach().numpy()\n",
    "    return torch.FloatTensor(-x/sigma)\n",
    "\n",
    "# Forward Loss\n",
    "def get_foward_loss_coupled(dB, init_x,NT, target,y0_model, z_models):\n",
    "    x =  init_x\n",
    "    # y = torch.rand_like(x)\n",
    "    y_tilde=y0_model(x)\n",
    "    y=torch.sigmoid(y_tilde)\n",
    "    for j in range(1, NT+1):\n",
    " \n",
    "        z = z_models[j-1](x)\n",
    "        x =x+ y*dt+ dB[:,j].view(-1,1)\n",
    "        y_tilde = (y_tilde +(z**2)*(1-2/(1+torch.exp(y_tilde)))/2*dt  + z * dB[:,j].view(-1,1))#.clamp(min=-1,max=1)\n",
    "        y=torch.sigmoid(y_tilde)\n",
    "       \n",
    "    loss=torch.mean((y_tilde-target(x))**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_target_path_coupled(dB, init_x,NumBM, NT,y0_model, z_models):\n",
    "    x_path = torch.ones(NumBM,NT+1)\n",
    "    y_path = torch.ones(NumBM,NT+1)\n",
    "    x = init_x\n",
    "    # y = torch.rand_like(x)\n",
    "    y_tilde=y0_model(x)\n",
    "    y=torch.sigmoid(y_tilde)\n",
    "    x_path[:,0] = x.squeeze()\n",
    "    y_path[:,0] = y.squeeze()\n",
    "    for j in range(1, NT+1):\n",
    "        z = z_models[j-1](x)\n",
    "        x += y*dt+ dB[:,j].view(-1,1)\n",
    "        y_tilde = (y_tilde +(z**2)*(1-2/(1+torch.exp(y_tilde)))/2 *dt + z * dB[:,j].view(-1,1))#.clamp(min=-1,max=1)\n",
    "        y=torch.sigmoid(y_tilde)\n",
    "        x_path[:,j] = x.squeeze()\n",
    "        y_path[:,j] = y.squeeze()\n",
    "    return x_path.detach(), y_path.detach()\n",
    "\n",
    "class plot_results():\n",
    "    def __init__(self,loss=forward_losses,sigma=sigma,Npaths=100,NumTrain=NumTrain,NT=NT):\n",
    "        self.loss=loss\n",
    "        self.x_path,self.y_path=get_target_path_coupled(dB, init_x, y0_model=y0_model_main, z_models=z_models_main, NumBM=NumTrain, NT=NT)\n",
    "        self.number_of_paths=np.minimum(Npaths,NumTrain)\n",
    "        self.sigma=sigma\n",
    "    \n",
    "    def FwdLoss(self,log=True):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.title(\"Forward_Loss vs Batch\",fontsize=18)\n",
    "        plt.plot(self.loss)\n",
    "\n",
    "        if log==True:\n",
    "            plt.yscale('log')\n",
    "\n",
    "    def results(self,seed=0):\n",
    "        random.seed(seed)\n",
    "        idx_list = np.random.choice(NumTrain, self.number_of_paths, replace = False)\n",
    "        x_plot = self.x_path.detach().numpy()[idx_list]\n",
    "        y_plot = self.y_path.detach().numpy()[idx_list]\n",
    "        t = np.array([i for i in range(NT+1)]) * 1/(NT)\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.subplot(121)\n",
    "        for i in range(self.number_of_paths):\n",
    "                plt.plot(t,x_plot[i], color=\"blue\", alpha=0.5)\n",
    "        plt.title(\"X\")\n",
    "\n",
    "        plt.subplot(122)\n",
    "        for i in range(self.number_of_paths):\n",
    "                plt.plot(t,y_plot[i], color=\"red\", alpha=0.5)\n",
    "        plt.title(\"Y Values\")\n",
    "\n",
    "        ### Integrated Plots\n",
    "        random.seed(seed)\n",
    "        idx=random.randint(0,self.number_of_paths)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.subplot()\n",
    "        plt.plot(t,x_plot[idx], color=\"blue\", alpha=0.5,label='X')\n",
    "        plt.plot(t,y_plot[idx], color=\"black\", linestyle='--',alpha=0.5,label=\"Y Values\")\n",
    "        plt.hlines(y=[0,1],xmin=0,xmax=1,colors='firebrick',linestyles='-.')\n",
    "        plt.title(\"Comparison of A Particular Path\")\n",
    "        plt.legend()\n",
    "    \n",
    "    def qq_plot(self,sigma=sigma):\n",
    "        plt.figure()\n",
    "        plt.title(\"QQ-Plot\")\n",
    "        x_sigmoid=1/(1+np.exp(self.x_path[:,-1]/sigma))\n",
    "        plt.scatter(x_sigmoid,self.y_path[:,-1],s=3)\n",
    "        plt.plot(np.linspace(0,1,5),np.linspace(0,1,5),linestyle='--',linewidth=1,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number:  1\n",
      "Average Error Est:  1181.9373413085937\n",
      "Batch Number:  2\n",
      "Average Error Est:  918.970751953125\n",
      "Batch Number:  3\n",
      "Average Error Est:  639.2976654052734\n",
      "Batch Number:  4\n",
      "Average Error Est:  366.01124114990233\n",
      "Batch Number:  5\n",
      "Average Error Est:  206.00214767456055\n",
      "Batch Number:  6\n",
      "Average Error Est:  120.03578338623046\n",
      "Batch Number:  7\n",
      "Average Error Est:  65.6522798538208\n",
      "Batch Number:  8\n",
      "Average Error Est:  38.98498239517212\n",
      "Batch Number:  9\n",
      "Average Error Est:  25.427432918548583\n",
      "Batch Number:  10\n",
      "Average Error Est:  18.020624542236327\n",
      "Batch Number:  11\n",
      "Average Error Est:  12.524469947814941\n",
      "Batch Number:  12\n",
      "Average Error Est:  9.08469762802124\n",
      "Batch Number:  13\n",
      "Average Error Est:  7.551621317863464\n",
      "Batch Number:  14\n",
      "Average Error Est:  6.943605828285217\n",
      "Batch Number:  15\n",
      "Average Error Est:  6.604528307914734\n",
      "Batch Number:  16\n",
      "Average Error Est:  6.373709487915039\n",
      "Batch Number:  17\n",
      "Average Error Est:  6.2264234781265255\n",
      "Batch Number:  18\n",
      "Average Error Est:  6.060656881332397\n",
      "Batch Number:  19\n",
      "Average Error Est:  5.937634778022766\n",
      "Batch Number:  20\n",
      "Average Error Est:  5.878761029243469\n",
      "Batch Number:  21\n",
      "Average Error Est:  5.839511346817017\n",
      "Batch Number:  22\n",
      "Average Error Est:  5.86887948513031\n",
      "Batch Number:  23\n",
      "Average Error Est:  5.933891153335571\n",
      "Batch Number:  24\n",
      "Average Error Est:  6.009975957870483\n",
      "Batch Number:  25\n",
      "Average Error Est:  6.076080393791199\n",
      "Batch Number:  26\n",
      "Average Error Est:  6.157733464241028\n",
      "Batch Number:  27\n",
      "Average Error Est:  6.170999121665955\n",
      "Batch Number:  28\n",
      "Average Error Est:  6.210317659378052\n",
      "Batch Number:  29\n",
      "Average Error Est:  6.184433627128601\n",
      "Batch Number:  30\n",
      "Average Error Est:  6.155380415916443\n",
      "Batch Number:  31\n",
      "Average Error Est:  6.146085810661316\n",
      "Batch Number:  32\n",
      "Average Error Est:  6.139582514762878\n",
      "Batch Number:  33\n",
      "Average Error Est:  6.216795444488525\n",
      "Batch Number:  34\n",
      "Average Error Est:  6.153911542892456\n",
      "Batch Number:  35\n",
      "Average Error Est:  6.126900339126587\n",
      "Batch Number:  36\n",
      "Average Error Est:  10.724046874046326\n",
      "Batch Number:  37\n",
      "Average Error Est:  6.129624843597412\n",
      "Batch Number:  38\n",
      "Average Error Est:  10.295892810821533\n",
      "Batch Number:  39\n",
      "Average Error Est:  6.137475323677063\n",
      "Batch Number:  40\n",
      "Average Error Est:  10.814829587936401\n",
      "Batch Number:  41\n",
      "Average Error Est:  6.157540941238404\n",
      "Batch Number:  42\n",
      "Average Error Est:  6.206046223640442\n",
      "Batch Number:  43\n",
      "Average Error Est:  6.124272894859314\n",
      "Batch Number:  44\n",
      "Average Error Est:  6.110557293891906\n",
      "Batch Number:  45\n",
      "Average Error Est:  6.095429611206055\n",
      "Batch Number:  46\n",
      "Average Error Est:  6.127217626571655\n",
      "Batch Number:  47\n",
      "Average Error Est:  6.081577229499817\n",
      "Batch Number:  48\n",
      "Average Error Est:  6.06583263874054\n",
      "Batch Number:  49\n",
      "Average Error Est:  6.064759278297425\n",
      "Batch Number:  50\n",
      "Average Error Est:  6.0791397333145145\n",
      "Batch Number:  51\n",
      "Average Error Est:  6.007607126235962\n",
      "Batch Number:  52\n",
      "Average Error Est:  5.991518330574036\n",
      "Batch Number:  53\n",
      "Average Error Est:  5.966437721252442\n",
      "Batch Number:  54\n",
      "Average Error Est:  5.911501169204712\n",
      "Batch Number:  55\n",
      "Average Error Est:  5.927151894569397\n",
      "Batch Number:  56\n",
      "Average Error Est:  5.9126629114151\n",
      "Batch Number:  57\n",
      "Average Error Est:  5.953045415878296\n",
      "Batch Number:  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Error detected in ExpBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/orangeao/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/xb/rvrt2lnd1bs3y0fbz2c5wbwm0000gn/T/ipykernel_1867/2372806750.py\", line 49, in <module>\n",
      "    loss = get_foward_loss_coupled(dB, init_x,NT=NT,target=target, y0_model=y0_model_main, z_models=z_models_main)\n",
      "  File \"/var/folders/xb/rvrt2lnd1bs3y0fbz2c5wbwm0000gn/T/ipykernel_1867/3736784659.py\", line 72, in get_foward_loss_coupled\n",
      "    y_tilde = (y_tilde +(z**2)*(1-2/(1+torch.exp(y_tilde)))/2*dt  + z * dB[:,j].view(-1,1))#.clamp(min=-1,max=1)\n",
      " (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1716905753263/work/torch/csrc/autograd/python_anomaly_mode.cpp:116.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'ExpBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_foward_loss_coupled(dB, init_x,NT\u001b[38;5;241m=\u001b[39mNT,target\u001b[38;5;241m=\u001b[39mtarget, y0_model\u001b[38;5;241m=\u001b[39my0_model_main, z_models\u001b[38;5;241m=\u001b[39mz_models_main)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(params)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters\u001b[38;5;241m=\u001b[39mparams,max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Anaconda/anaconda3/envs/env_py_311/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'ExpBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "## Train \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "dB = SampleBMIncr(1, Npaths=NumTrain, Nsteps=NT+1)\n",
    "init_x =  Sample_Init(N=NumTrain)\n",
    "\n",
    "#Forward Loss\n",
    "forward_losses = []\n",
    "#How many batches?\n",
    "MaxBatch= 500\n",
    "\n",
    "#How many optimization steps per batch\n",
    "OptimSteps= 20\n",
    "\n",
    "#Set Learning rate\n",
    "learning_rate = 0.005\n",
    "\n",
    "#Train on a single batch?\n",
    "single_batch = True\n",
    "\n",
    "#Set up main models for y0 and z (z will be list of models)\n",
    "layer_dim = 10\n",
    "y0_model_main = Network(lr=learning_rate, input_dims=[1], fc1_dims=layer_dim, fc2_dims=layer_dim,\n",
    "                     n_outputs=1)\n",
    "z_models_main = [Network(lr=learning_rate, input_dims=[1], fc1_dims=layer_dim, fc2_dims=layer_dim,\n",
    "                     n_outputs=1) for i in range(NT)]\n",
    "\n",
    "\n",
    "\n",
    "#Define optimization parameters\n",
    "# params = list(y0_model_main.parameters())\n",
    "params=[]\n",
    "for i in range(NT):\n",
    "    params += list(z_models_main[i].parameters())\n",
    "    \n",
    "#Set up optimizer and scheduler\n",
    "optimizer = optim.AdamW(params, lr=learning_rate,weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.99)\n",
    "\n",
    "\n",
    "for k in range(0,MaxBatch):\n",
    "\n",
    "    print(\"Batch Number: \", k+1)\n",
    "    sloss=0\n",
    "    #optimize main network wrt the foward loss\n",
    "    for l in range(0,OptimSteps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = get_foward_loss_coupled(dB, init_x,NT=NT,target=target, y0_model=y0_model_main, z_models=z_models_main)\n",
    "        # print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        # print(params)\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=params,max_norm=1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        nloss = loss.detach().numpy()\n",
    "        sloss += nloss\n",
    "        # print('OptimStep: '+ str(l+1))\n",
    "        # print('forward_loss: ' + str(nloss))\n",
    "    avgloss = sloss/OptimSteps\n",
    "    print(\"Average Error Est: \", avgloss)\n",
    "    forward_losses.append(avgloss)\n",
    "\n",
    "    #Generate a new batch if using multiple batches\n",
    "    if(not single_batch):\n",
    "        dB = SampleBMIncr(1, Npaths=NumTrain, Nsteps=NT+1)\n",
    "        init_x =  Sample_Init(N=NumTrain)\n",
    "\n",
    "\n",
    "plot=plot_results(loss=forward_losses)\n",
    "plot.FwdLoss()\n",
    "plot.results()\n",
    "plot.qq_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
